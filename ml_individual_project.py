# -*- coding: utf-8 -*-
"""Copy of ML Individual Project_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zX0paUzOzX85AUxzHAJ_igWfPwxhFq1_

# **Personalized Telecom Plan Recommendation Using Customer Segmentation and Churn Prediction**

# **Data Loading**

Importing Relevant Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import (classification_report, confusion_matrix,
                             silhouette_score, davies_bouldin_score,
                             calinski_harabasz_score, roc_auc_score, roc_curve,
                             accuracy_score, precision_score, recall_score, f1_score)
import warnings
warnings.filterwarnings('ignore')

#Set visualization style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

telco = pd.read_csv('data/telco.csv')
telco.head()

print(f"\nDataset Shape: {telco.shape}")
print(f" Rows: {telco.shape[0]:,}")
print(f" Columns: {telco.shape[1]:,}")

print("\n Column names and types: ")
print(telco.dtypes)

print("\n Basic Statistics (Numeric columns):")
print(telco.describe())

telco['Total Charges'].dtype
telco['Total Charges'].unique()[:10]

telco['Total Charges'] = telco['Total Charges'].str.strip()

telco['Total Charges'] = pd.to_numeric(telco['Total Charges'], errors='coerce')

telco.loc[telco['Total Charges'].isna(), ['Tenure Months', 'Monthly Charges', 'Total Charges']]

"""Checking missing data"""

print("\n Missing Values Analysis: ")
missing = telco.isnull().sum()
print(missing)
missing_pct = (missing/len(telco)) * 100
missing_df = pd.DataFrame({'Missing_count': missing, 'Percentage': missing_pct}).sort_values('Missing_count', ascending=False)

missing_with_values = missing_df[missing_df['Missing_count'] > 0]
if len(missing_with_values) > 0:
  print(missing_with_values)
else:
  print("   No missing values found!")

"""Imputing missing values"""

#imputing missing values
telco['Churn Reason'] = telco['Churn Reason'].fillna('Not Churned')

telco.iloc[1870:1875]

#median imputation for Total Charges
telco['Total Charges'].fillna(telco['Total Charges'].median(), inplace=True)

miss = telco.isnull().sum()
print(miss)

"""# **Exploratory Data Analysis (EDA)**

Churn Distribution
"""

# sns.countplot(x="Churn Label", data=telco)
# plt.title("Churn Distribution")
# plt.show()



# Churn Distribution
print("\n Churn Distribution:")

if 'Churn Label' in telco.columns:
  churn_dist = telco['Churn Label'].value_counts()
  churn_pct = telco['Churn Label'].value_counts(normalize=True) * 100
  print(f"   Retained (No): {churn_dist.get('No', 0):,} customers ({churn_pct.get('No', 0):.2f}%)")
  print(f"   Churned (Yes): {churn_dist.get('Yes', 0):,} customers ({churn_pct.get('Yes', 0):.2f}%)")
  print(f"   Churn Rate: {churn_pct.get('Yes', 0):.2f}%")

  # Visualize churn distribution
  fig, axes = plt.subplots(1, 2, figsize=(14, 5))

  # Pie chart
  colors = ['#10b981', '#ef4444']
  axes[0].pie(churn_dist.values, labels=churn_dist.index, autopct='%1.1f%%',
                    colors=colors, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})
  axes[0].set_title('Churn Distribution (Pie Chart)', fontsize=14, fontweight='bold')

  # Bar chart
  churn_dist.plot(kind='bar', ax=axes[1], color=colors, edgecolor='black')
  axes[1].set_xlabel('Churn Status', fontsize=12)
  axes[1].set_ylabel('Number of Customers', fontsize=12)
  axes[1].set_title('Churn Distribution (Bar Chart)', fontsize=14, fontweight='bold')
  axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)

  # Add value labels on bars
  for i, v in enumerate(churn_dist.values):
    axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    # plt.savefig('eda_plots/02_churn_distribution.png', dpi=300, bbox_inches='tight')
    # print(" Saved: eda_plots/02_churn_distribution.png")
    plt.show()

"""Numeric Feature Analysis"""

# Numeric Features Analysis
print("\n Numeric Features Summary:")

numeric_cols = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'CLTV']
available_numeric = [col for col in numeric_cols if col in telco.columns]

for col in available_numeric:
  print(f"\n{col}:")
  print(f"   Mean: {telco[col].mean():.2f}")
  print(f"   Median: {telco[col].median():.2f}")
  print(f"   Std Dev: {telco[col].std():.2f}")
  print(f"   Min: {telco[col].min():.2f}")
  print(f"   Max: {telco[col].max():.2f}")

if available_numeric:
  # Distribution plots for numeric features
  fig, axes = plt.subplots(2, 2, figsize=(14, 10))
  axes = axes.flatten()

  for idx, col in enumerate(available_numeric[:4]):
    if col in telco.columns:
       # Histogram with KDE
      axes[idx].hist(telco[col].dropna(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')
      axes[idx].set_xlabel(col, fontsize=11)
      axes[idx].set_ylabel('Frequency', fontsize=11)
      axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')
      axes[idx].grid(axis='y', alpha=0.3)

  plt.tight_layout()
  plt.show()

        # Box plots for numeric features by churn
  if 'Churn Label' in telco.columns:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for idx, col in enumerate(available_numeric[:4]):
      if col in telco.columns:
        telco.boxplot(column=col, by='Churn Label', ax=axes[idx],
                              patch_artist=True, grid=False)
        axes[idx].set_xlabel('Churn Status', fontsize=11)
        axes[idx].set_ylabel(col, fontsize=11)
        axes[idx].set_title(f'{col} by Churn Status', fontsize=12, fontweight='bold')
        plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=0)

    plt.suptitle('')  # Remove default title
    plt.tight_layout()
    plt.show()

"""Categorical Features Distribution"""

# Categorical Features Distribution
print("\n Key Categorical Features Distribution:")
cat_features = ['Contract', 'Internet Service', 'Payment Method']
for col in cat_features:
  if col in telco.columns:
    print(f"\n{col}:")
    print(telco[col].value_counts())

"""Contract Type Vs Churn"""

# Contract Type vs Churn
print("\n Contract Type vs Churn Rate:")

if 'Contract' in telco.columns and 'Churn Value' in telco.columns:
  churn_by_contract = telco.groupby('Contract').agg({
      'Churn Value': ['mean', 'count']
  })
  churn_by_contract.columns = ['Churn_Rate', 'Count']
  churn_by_contract['Churn_Rate_%'] = churn_by_contract['Churn_Rate'] * 100
  print(churn_by_contract)

  # Visualize Contract vs Churn
  fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Churn rate by contract
  churn_by_contract['Churn_Rate_%'].plot(kind='bar', ax=axes[0], color='indianred',
                                                 edgecolor='black', alpha=0.8)
  axes[0].set_xlabel('Contract Type', fontsize=11)
  axes[0].set_ylabel('Churn Rate (%)', fontsize=11)
  axes[0].set_title('Churn Rate by Contract Type', fontsize=12, fontweight='bold')
  axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')
  axes[0].grid(axis='y', alpha=0.3)

        # Add value labels
  for i, v in enumerate(churn_by_contract['Churn_Rate_%'].values):
    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')

        # Stacked bar chart
    contract_churn = pd.crosstab(telco['Contract'], telco['Churn Label'])
    contract_churn.plot(kind='bar', stacked=True, ax=axes[1], color=['#10b981', '#ef4444'],
                            edgecolor='black', alpha=0.8)
    axes[1].set_xlabel('Contract Type', fontsize=11)
    axes[1].set_ylabel('Number of Customers', fontsize=11)
    axes[1].set_title('Contract Type Distribution by Churn Status', fontsize=12, fontweight='bold')
    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')
    axes[1].legend(title='Churn Status')

    plt.tight_layout()
    plt.show()

# Internet Service vs Churn
print("\n Internet Service vs Churn Rate:")

if 'Internet Service' in telco.columns and 'Churn Value' in telco.columns:
  churn_by_internet = telco.groupby('Internet Service').agg({
      'Churn Value': ['mean', 'count']
  })
  churn_by_internet.columns = ['Churn_Rate', 'Count']
  churn_by_internet['Churn_Rate_%'] = churn_by_internet['Churn_Rate'] * 100
  print(churn_by_internet)

        # Visualize Internet Service vs Churn
  fig, ax = plt.subplots(figsize=(10, 6))
  churn_by_internet['Churn_Rate_%'].plot(kind='bar', ax=ax, color='mediumpurple', edgecolor='black', alpha=0.8)
  ax.set_xlabel('Internet Service Type', fontsize=11)
  ax.set_ylabel('Churn Rate (%)', fontsize=11)
  ax.set_title('Churn Rate by Internet Service Type', fontsize=12, fontweight='bold')
  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
  ax.grid(axis='y', alpha=0.3)

        # Add value labels
  for i, v in enumerate(churn_by_internet['Churn_Rate_%'].values):
    ax.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')

  plt.tight_layout()
  plt.show()

"""Tenure Vs Churn"""

# Tenure vs Churn
if 'Tenure Months' in telco.columns and 'Churn Label' in telco.columns:
  fig, axes = plt.subplots(1, 2, figsize=(14, 5))

  # Distribution by churn
  for label in telco['Churn Label'].unique():
    subset = telco[telco['Churn Label'] == label]['Tenure Months']
    axes[0].hist(subset, bins=30, alpha=0.6, label=f'Churn: {label}', edgecolor='black')
  axes[0].set_xlabel('Tenure (Months)', fontsize=11)
  axes[0].set_ylabel('Frequency', fontsize=11)
  axes[0].set_title('Tenure Distribution by Churn Status', fontsize=12, fontweight='bold')
  axes[0].legend()
  axes[0].grid(axis='y', alpha=0.3)

  # Violin plot
  sns.violinplot(data=telco, x='Churn Label', y='Tenure Months', ax=axes[1], palette='Set2')
  axes[1].set_xlabel('Churn Status', fontsize=11)
  axes[1].set_ylabel('Tenure (Months)', fontsize=11)
  axes[1].set_title('Tenure Distribution by Churn (Violin Plot)', fontsize=12, fontweight='bold')

  plt.tight_layout()
  plt.show()

"""Tenure Distribution"""

sns.histplot(telco["Tenure Months"], bins=30, kde=True)
plt.title("Tenure Distribution")
plt.show()

"""Monthly Charges Vs Churn"""

# Monthly Charges vs Churn
if 'Monthly Charges' in telco.columns and 'Churn Label' in telco.columns:
  fig, ax = plt.subplots(figsize=(10, 6))

  for label in telco['Churn Label'].unique():
    subset = telco[telco['Churn Label'] == label]['Monthly Charges']
    ax.hist(subset, bins=30, alpha=0.6, label=f'Churn: {label}', edgecolor='black')

  ax.set_xlabel('Monthly Charges ($)', fontsize=11)
  ax.set_ylabel('Frequency', fontsize=11)
  ax.set_title('Monthly Charges Distribution by Churn Status', fontsize=12, fontweight='bold')
  ax.legend()
  ax.grid(axis='y', alpha=0.3)

  plt.tight_layout()
  plt.show()

"""Geographic Analysis"""

sns.scatterplot(
    x="Longitude",
    y="Latitude",
    hue="Churn Label",
    data=telco,
    alpha=0.5
)
plt.title("Customer Location vs Churn")
plt.show()

"""Payment Method Vs Churn"""

sns.countplot(x="Payment Method", hue="Churn Label", data=telco)
plt.xticks(rotation=45)
plt.title("Churn by Payment Method")
plt.show()

"""PairPlot"""

pairplot_features = [
    "Tenure Months",
    "Monthly Charges",
    "Total Charges",
    "CLTV"
]


sns.pairplot(
    telco[pairplot_features + ["Churn Label"]],
    hue="Churn Label",
    diag_kind="kde",
    corner=True
)
plt.show()

"""Correlation Analysis"""

print("\n Correlation with Churn:")

numeric_features = [
    "Tenure Months",
    "Monthly Charges",
    "Total Charges",
    "Churn Score",
    "CLTV",
    "Churn Value"
]

# Ensure columns exist
numeric_features = [col for col in numeric_features if col in telco.columns]

correlations = (
    telco[numeric_features]
    .corr()["Churn Value"]
    .sort_values(ascending=False)
)

print(correlations)

# Correlation matrix
corr_matrix = telco[numeric_features].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(
    corr_matrix,
    annot=True,
    fmt='.2f',
    cmap='coolwarm',
    center=0,
    cbar_kws={'label': 'Correlation'}
)

plt.title(
    'Correlation Matrix (All Numeric Features)',
    fontsize=12,
    fontweight='bold'
)
plt.grid(False)
plt.tight_layout()
plt.show()

"""# **Data Preprocessing**

Feature Engineering
"""

# Feature Engineering
print("\n Feature Engineering:")

#Tenure groups
if 'Tenure Months' in telco.columns:
  telco['Tenure_Group'] = pd.cut(telco['Tenure Months'], bins=[0, 12, 24, 48, 72, np.inf], labels=['0-1yr', '1-2yr', '2-4yr', '4-6yr', '6+yr'])
  print(f" Created 'Tenure_Group' feature with 5 categories")
  print(f" Distribution:")
  print(telco['Tenure_Group'].value_counts().to_string())

    #  Charges per month (avoiding division by zero)
if 'Total Charges' in telco.columns and 'Tenure Months' in telco.columns:
  telco['Avg_Monthly_Spend'] = telco['Total Charges'] / (telco['Tenure Months'] + 1)
  print(f"\n Created 'Avg_Monthly_Spend' feature")
  print(f" Mean: ${telco['Avg_Monthly_Spend'].mean():.2f}")
  print(f" Median: ${telco['Avg_Monthly_Spend'].median():.2f}")

    # Total services count
service_cols = ['Phone Service', 'Internet Service', 'Online Security',
                'Online Backup', 'Device Protection', 'Tech Support',
                'Streaming TV', 'Streaming Movies']
telco['Total_Services'] = 0
for col in service_cols:
  if col in telco.columns:
    telco['Total_Services'] += (telco[col] == 'Yes').astype(int)
print(f"\n Created 'Total_Services' feature (count of active services)")
print(f" Mean services per customer: {telco['Total_Services'].mean():.2f}")
print(f" Distribution:")
print(telco['Total_Services'].value_counts().sort_index().to_string())

    # Is Senior with dependents
if 'Senior Citizen' in telco.columns and 'Dependents' in telco.columns:
  telco['Senior_With_Dependents'] = ((telco['Senior Citizen'] == 'Yes') &
                                                    (telco['Dependents'] == 'Yes')).astype(int)
  print(f"\n Created 'Senior_With_Dependents' feature")
  print(f" Count: {telco['Senior_With_Dependents'].sum()}")

    # Contract length indicator
if 'Contract' in telco.columns:
  telco['Long_Term_Contract'] = (telco['Contract'].isin(['One year', 'Two year'])).astype(int)
  print(f"\n Created 'Long_Term_Contract' feature")
  print(f" Long-term contracts: {telco['Long_Term_Contract'].sum()} ({telco['Long_Term_Contract'].mean()*100:.1f}%)")

# =============================================================================
# ADDITIONAL ADVANCED FEATURES - INSERT AFTER YOUR EXISTING FEATURE ENGINEERING
# =============================================================================

print("\n Creating Additional Advanced Features:")

# 1. Service engagement score (high value = engaged customer)
telco['Service_Engagement_Score'] = (
    telco['Total_Services'] * telco['Tenure Months'] /
    (telco['Monthly Charges'] + 1)
)
print(f" Created 'Service_Engagement_Score'")

# 2. Price per service (high value = expensive relative to services)
telco['Price_Per_Service'] = telco['Monthly Charges'] / (telco['Total_Services'] + 1)
print(f" Created 'Price_Per_Service'")

# 3. Early churner risk indicator (customers < 12 months are high risk)
telco['Early_Churner_Risk'] = (telco['Tenure Months'] < 12).astype(int)
print(f" Created 'Early_Churner_Risk'")

# 4. High charge + short contract (dangerous combination)
if 'Contract' in telco.columns:
    median_charge = telco['Monthly Charges'].median()
    telco['High_Charge_Short_Contract'] = (
        (telco['Contract'] == 'Month-to-month') &
        (telco['Monthly Charges'] > median_charge)
    ).astype(int)
    print(f" Created 'High_Charge_Short_Contract'")

# 5. Risky payment method (Electronic check has highest churn)
if 'Payment Method' in telco.columns:
    telco['Risky_Payment_Method'] = (
        telco['Payment Method'] == 'Electronic check'
    ).astype(int)
    print(f" Created 'Risky_Payment_Method'")

# 6. Count of missing/inactive services
service_cols = ['Online Security', 'Online Backup', 'Device Protection',
                'Tech Support', 'Streaming TV', 'Streaming Movies']
telco['Missing_Services_Count'] = 0
for col in service_cols:
    if col in telco.columns:
        telco['Missing_Services_Count'] += (telco[col] != 'Yes').astype(int)
print(f" Created 'Missing_Services_Count'")

# 7. CLTV per month (normalized customer value)
telco['CLTV_Per_Month'] = telco['CLTV'] / (telco['Tenure Months'] + 1)
print(f" Created 'CLTV_Per_Month'")

# 8. Fiber optic + high charges (specific high-risk combination)
if 'Internet Service' in telco.columns:
    telco['Fiber_High_Charge'] = (
        (telco['Internet Service'] == 'Fiber optic') &
        (telco['Monthly Charges'] > telco['Monthly Charges'].median())
    ).astype(int)
    print(f" Created 'Fiber_High_Charge'")

# 9. Isolated senior (senior without partner or dependents)
if 'Senior Citizen' in telco.columns and 'Partner' in telco.columns and 'Dependents' in telco.columns:
    telco['Isolated_Senior'] = (
        (telco['Senior Citizen'] == 1) &
        (telco['Partner'] == 0) &
        (telco['Dependents'] == 0)
    ).astype(int)
    print(f" Created 'Isolated_Senior'")

# 10. Tenure to charges ratio (value received over time)
telco['Tenure_Charge_Ratio'] = telco['Tenure Months'] / (telco['Monthly Charges'] + 1)
print(f" Created 'Tenure_Charge_Ratio'")

print(f"\n Total new features created: 10")
print(f" Enhanced dataset shape: {telco.shape}")

"""Encode Categorical Variables"""

print("\n Encoding Strategy:")

# ONE-HOT encoding (nominal, no order)
onehot_features = ['Contract', 'Payment Method', 'Internet Service']

# LABEL encoding (binary / low cardinality)
label_encode_features = [
    'Gender', 'Senior Citizen', 'Partner', 'Dependents',
    'Phone Service', 'Multiple Lines', 'Online Security',
    'Online Backup', 'Device Protection', 'Tech Support',
    'Streaming TV', 'Streaming Movies', 'Paperless Billing'
]

print(f" One-Hot Encoding ({len(onehot_features)}): {onehot_features}")
print(f" Label Encoding ({len(label_encode_features)}): {label_encode_features}")



print("\n Label Encoding (0/1):")

label_encoders = {}
for col in label_encode_features:
    if col in telco.columns:
        le = LabelEncoder()
        telco[col] = le.fit_transform(telco[col].astype(str))  # ðŸ”¥ REPLACE column
        label_encoders[col] = le
        print(f" Encoded '{col}' â†’ values: {list(le.classes_)}")


print("\n One-Hot Encoding (0/1):")

for col in onehot_features:
    if col in telco.columns:
        dummies = pd.get_dummies(
            telco[col],
            prefix=col.replace(" ", "_"),
            dtype=int
        )

        telco = pd.concat([telco.drop(columns=[col]), dummies], axis=1)

        print(f" One-hot encoded '{col}' â†’ {len(dummies.columns)} columns")
        print(f" Columns: {', '.join(dummies.columns)}")



print(f"\n Final Dataset Shape: {telco.shape}")
print(" All encoded features are numeric (0/1)")
print(" No duplicate categorical columns remain")

telco.head()

# print("\n Feature Selection:")

# feature_cols = [col for col in telco.columns if col.endswith('_encoded') or
#                 col in ['Tenure Months', 'Monthly Charges', 'Total Charges',
#                         'CLTV', 'Total_Services', 'Avg_Monthly_Spend',
#                         'Senior_With_Dependents', 'Long_Term_Contract'] or
#                 col.startswith('Contract_') or col.startswith('Payment_') or
#                 col.startswith('Internet_')]

# feature_cols = [col for col in feature_cols if col in telco.columns]

# print(f" Total features selected: {len(feature_cols)}")
# print(f" First 15 features: {feature_cols[:15]}")
# print(f"......")

# =============================================================================
# FEATURE SELECTION
# =============================================================================

print("\n Feature Selection:")

# Include ALL potential features (original + new ones)
feature_cols = [
    # Original numeric features
    'Tenure Months', 'Monthly Charges', 'Total Charges', 'CLTV',

    # Original engineered features
    'Total_Services', 'Avg_Monthly_Spend', 'Senior_With_Dependents',
    'Long_Term_Contract',

    # NEW advanced features
    'Service_Engagement_Score', 'Price_Per_Service', 'Early_Churner_Risk',
    'High_Charge_Short_Contract', 'Risky_Payment_Method', 'Missing_Services_Count',
    'CLTV_Per_Month', 'Fiber_High_Charge', 'Isolated_Senior', 'Tenure_Charge_Ratio'
]

# Add all encoded categorical features
categorical_encoded = [
    # Label encoded features
    'Gender', 'Senior Citizen', 'Partner', 'Dependents',
    'Phone Service', 'Multiple Lines', 'Online Security',
    'Online Backup', 'Device Protection', 'Tech Support',
    'Streaming TV', 'Streaming Movies', 'Paperless Billing'
]

# Add one-hot encoded features (Contract, Payment Method, Internet Service)
onehot_cols = [col for col in telco.columns if
               col.startswith('Contract_') or
               col.startswith('Payment_Method_') or
               col.startswith('Internet_Service_')]

# Combine all features
feature_cols.extend(categorical_encoded)
feature_cols.extend(onehot_cols)

# Keep only features that exist in the dataframe
feature_cols = [col for col in feature_cols if col in telco.columns]

print(f" Total candidate features: {len(feature_cols)}")

# Remove low-variance features and highly correlated features
from sklearn.feature_selection import VarianceThreshold

X_temp = telco[feature_cols].fillna(0)

# Remove features with very low variance
selector = VarianceThreshold(threshold=0.01)
selector.fit(X_temp)
high_variance_features = X_temp.columns[selector.get_support()].tolist()

print(f" After variance threshold: {len(high_variance_features)} features")

# Remove highly correlated features (keep correlation < 0.95)
corr_matrix = X_temp[high_variance_features].corr().abs()
upper_triangle = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

to_drop = [column for column in upper_triangle.columns
           if any(upper_triangle[column] > 0.95)]

final_features = [f for f in high_variance_features if f not in to_drop]

print(f" After correlation filter: {len(final_features)} features")
print(f" Removed {len(to_drop)} highly correlated features")

# Update feature_cols
feature_cols = final_features

print(f"\n Final selected features: {len(feature_cols)}")
print(f" First 20 features: {feature_cols[:20]}")
if len(feature_cols) > 20:
    print(f" ... and {len(feature_cols) - 20} more")



"""# **Scaling & Train/Test Split**"""

x = telco[feature_cols].fillna(0)
y = telco["Churn Value"] if 'Churn Value' in telco.columns else (telco['Churn Label'] == 'Yes').astype(int)

print(f"   Features (X) shape: {x.shape}")
print(f"   Target (y) shape: {y.shape}")
print(f"   Positive class (Churned): {y.sum()} ({y.mean():.2%})")
print(f"   Negative class (Retained): {len(y) - y.sum()} ({(1-y.mean()):.2%})")

"""Train-Test Split"""

print(f"Train-Test Split")
x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, random_state=42, stratify=y
    )
print(f"   Training set: {x_train.shape[0]} samples ({x_train.shape[0]/len(x)*100:.1f}%)")
print(f"   Testing set: {x_test.shape[0]} samples ({x_test.shape[0]/len(x)*100:.1f}%)")
print(f"   Training churn rate: {y_train.mean():.2%}")
print(f"   Testing churn rate: {y_test.mean():.2%}")

"""Scale Features"""

print(f"\n Feature Scaling:")

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)
print("Features scaled using StandardScaler (mean=0, std=1)")
print(f"Scaler fitted on training data and applied to test data")

"""# **Model Training and Evaluation**

Shared Evaluation Function
"""

def evaluate_model(name, model, x_train,x_test, y_train, y_test):
    print(f" MODEL EVALUATION: {name}")


    y_pred = model.predict(x_test)
    y_proba = model.predict_proba(x_test)[:, 1]

    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "auc": roc_auc_score(y_test, y_proba)
    }

    for k, v in metrics.items():
        print(f"{k.capitalize():<10}: {v:.4f}")

    print("\n Classification Report:")
    print(classification_report(y_test, y_pred, target_names=["Retained", "Churned"]))

    print("\n Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring="roc_auc")
    metrics["cv_auc_mean"] = cv_scores.mean()
    metrics["cv_auc_std"] = cv_scores.std()

    print(f"\n CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

    return metrics

"""Training Models

*Logistic Regression*
"""

def train_logistic_regression(x_train, x_test, y_train, y_test):
    model = LogisticRegression(
        max_iter=1000,
        class_weight="balanced",
        random_state=42
    )
    model.fit(x_train, y_train)

    results = evaluate_model(
        "Logistic Regression",
        model,
        x_train, x_test,
        y_train, y_test
    )

    return model, results

"""*Random Forest*"""

def train_random_forest(x_train, x_test, y_train, y_test, feature_cols):
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        # min_samples_split=5,
        min_samples_leaf=2,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )
    model.fit(x_train, y_train)

    results = evaluate_model(
        "Random Forest",
        model,
        x_train, x_test,
        y_train, y_test
    )

    importance = (
        pd.DataFrame({
            "feature": feature_cols,
            "importance": model.feature_importances_
        })
        .sort_values("importance", ascending=False)
        .head(15)
    )

    print("\n Top 15 Feature Importances:")
    print(importance)

    results["feature_importance"] = importance
    return model, results

"""*XG Boost*"""

def train_xgboost(x_train, x_test, y_train, y_test, feature_cols):
    scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()

    model = XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        eval_metric="logloss",
        random_state=42
    )
    model.fit(x_train, y_train)

    results = evaluate_model(
        "XGBoost",
        model,
        x_train, x_test,
        y_train, y_test
    )

    importance = (
        pd.DataFrame({
            "feature": feature_cols,
            "importance": model.feature_importances_
        })
        .sort_values("importance", ascending=False)
        .head(15)
    )

    print("\n Top 15 Feature Importances:")
    print(importance)

    results["feature_importance"] = importance
    return model, results

# ===================
# ADD LIGHTGBM MODEL
# =====================


from lightgbm import LGBMClassifier

def train_lightgbm(x_train, x_test, y_train, y_test, feature_cols):
    """
    Train LightGBM model (often outperforms XGBoost for tabular data)
    """
    print(f"\n{'='*70}")
    print("TRAINING: LightGBM")
    print(f"{'='*70}")

    model = LGBMClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        num_leaves=60,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_samples=20,
        reg_alpha=0.1,
        reg_lambda=0.1,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1,
        verbose=-1  # Suppress output
    )

    model.fit(x_train, y_train)

    results = evaluate_model(
        "LightGBM",
        model,
        x_train, x_test,
        y_train, y_test
    )

    # Feature importance
    importance = (
        pd.DataFrame({
            "feature": feature_cols,
            "importance": model.feature_importances_
        })
        .sort_values("importance", ascending=False)
        .head(15)
    )

    print("\n Top 15 Feature Importances:")
    print(importance.to_string(index=False))

    results["feature_importance"] = importance
    return model, results

models = {}
results = {}

models["Logistic Regression"], results["Logistic Regression"] = \
    train_logistic_regression(x_train_scaled, x_test_scaled, y_train, y_test)

models["Random Forest"], results["Random Forest"] = \
    train_random_forest(x_train_scaled, x_test_scaled, y_train, y_test, feature_cols)

models["XGBoost"], results["XGBoost"] = \
    train_xgboost(x_train_scaled, x_test_scaled, y_train, y_test, feature_cols)

models["LightGBM"], results["XGBoost"] = \
    train_lightgbm(x_train_scaled, x_test_scaled, y_train, y_test, feature_cols)

"""Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, model_name):
    """
    Plots a confusion matrix heatmap
    """
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(5, 4))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        cbar=False,
        xticklabels=['Retained', 'Churned'],
        yticklabels=['Retained', 'Churned']
    )
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix â€“ {model_name}')
    plt.tight_layout()
    plt.show()
# plot_confusion_matrix(y_true, y_pred, model_name)

"""*Model Comparison*"""

comparison_df = pd.DataFrame(results).T[
    ["accuracy", "precision", "recall", "f1", "auc", "cv_auc_mean"]
]

print("\n FINAL MODEL COMPARISON")
print(comparison_df)

best_model_name = comparison_df["auc"].idxmax()
best_model = models[best_model_name]

print(f"\n Best Model Selected: {best_model_name}")

# =======================
# IMPROVED RESAMPLING
# =======================



from imblearn.combine import SMOTETomek
from imblearn.over_sampling import BorderlineSMOTE

print(f"\n{'='*70}")
print("ADVANCED RESAMPLING: SMOTETomek")
print(f"{'='*70}")

print(f"\nOriginal distribution:")
print(f"  Training samples: {len(y_train)}")
print(f"  Class 0 (No Churn): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)")
print(f"  Class 1 (Churn): {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)")
print(f"  Imbalance ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1")

# SMOTETomek: SMOTE oversampling + Tomek links undersampling
# This cleans borderline cases and improves model generalization
smote_tomek = SMOTETomek(random_state=42)
x_train_res, y_train_res = smote_tomek.fit_resample(x_train_scaled, y_train)

print(f"\nAfter SMOTETomek:")
print(f"  Training samples: {len(y_train_res)} (was {len(y_train)})")
print(f"  Class 0 (No Churn): {(y_train_res == 0).sum()}")
print(f"  Class 1 (Churn): {(y_train_res == 1).sum()}")
print(f"  New balance ratio: {(y_train_res == 0).sum() / (y_train_res == 1).sum():.2f}:1")
print(f"  Change: +{len(y_train_res) - len(y_train)} samples")

"""# **Model Optimisation**"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import (
        accuracy_score, precision_score, recall_score,
        f1_score, roc_auc_score, confusion_matrix,
        classification_report, precision_recall_curve
    )




import matplotlib.pyplot as plt
import seaborn as sns

def build_and_compare_churn_models_optimized(telco):
    """
    Optimized churn prediction pipeline with:
    - Hyperparameter tuning
    - Threshold optimization
    - Confusion matrix plots
    """


    results = {}
    models = {}

    # ------------------------------------------------------------------
    # Helper: Threshold Optimization
    # ------------------------------------------------------------------
    def optimize_threshold(y_true, y_prob):
        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
        best_idx = np.argmax(f1_scores)
        return thresholds[best_idx], f1_scores[best_idx]

    # ------------------------------------------------------------------
    # Helper: Plot confusion matrix
    # ------------------------------------------------------------------
    def plot_conf_matrix(y_true, y_pred, model_name):
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(5,4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title(f'Confusion Matrix: {model_name}')
        plt.show()

    # ==================================================================
    # MODEL 1: LOGISTIC REGRESSION
    # ==================================================================
    print("\n Logistic Regression (Tuned)")

    lr_params = {
        'C': [0.01, 0.1, 0.5, 1.0],  # More granular
        'penalty': ['l1', 'l2'],  # Try both regularizations
        'solver': ['liblinear'],  # Solvers that support l1
        'class_weight': ['balanced'],
        'max_iter': [500]  # Increased for convergence
    }

    lr = LogisticRegression(random_state=42)
    lr_cv = RandomizedSearchCV(
        lr, lr_params, n_iter=30, scoring='f1', cv=3, n_jobs=-1,  random_state=42
    )
    lr_cv.fit(x_train_res, y_train_res)

    lr_best = lr_cv.best_estimator_
    y_prob = lr_best.predict_proba(x_test_scaled)[:, 1]

    best_thresh, best_f1 = optimize_threshold(y_test, y_prob)
    y_pred = (y_prob >= best_thresh).astype(int)

    plot_conf_matrix(y_test, y_pred, "Logistic Regression")

    results['Logistic Regression'] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_prob),
        'threshold': best_thresh
    }

    models['Logistic Regression'] = lr_best

    # ==================================================================
    # MODEL 2: RANDOM FOREST
    # ==================================================================
    print("\n Random Forest (Tuned)")

    rf_params = {
        'n_estimators': [100, 200, 300],  # More trees
        'max_depth': [8, 12, None],  # Deeper trees
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2],
        'max_features': ['sqrt', 'log2'],  # Feature sampling strategies
        'class_weight': ['balanced'],  # Try both
        'bootstrap': [True],
        'criterion': ['gini', 'entropy']  # Try both split criteria
    }

    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    rf_cv = RandomizedSearchCV(
        rf, rf_params, n_iter=40, scoring='f1', cv=3, n_jobs=-1, random_state=42
    )
    rf_cv.fit(x_train_res, y_train_res)

    rf_best = rf_cv.best_estimator_
    y_prob = rf_best.predict_proba(x_test_scaled)[:, 1]

    best_thresh, best_f1 = optimize_threshold(y_test, y_prob)
    y_pred = (y_prob >= best_thresh).astype(int)

    plot_conf_matrix(y_test, y_pred, "Random Forest")

    results['Random Forest'] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_prob),
        'threshold': best_thresh
    }

    models['Random Forest'] = rf_best

    # ==================================================================
    # MODEL 3: XGBOOST
    # ==================================================================
    print("\n XGBoost (Tuned)")

    scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()

    xgb_params = {
        'n_estimators': [100, 200, 300],
        'max_depth': [4, 6, 8],
        'learning_rate': [0.01, 0.03, 0.05],
        'subsample': [0.7, 0.8],
        'colsample_bytree': [0.7, 0.8],
        'min_child_weight': [1, 3, 5, 7],
        # 'gamma': [0, 0.1, 0.5, 1.0],  # Regularization
        # 'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization
        # 'reg_lambda': [0.5, 1.0, 2.0, 5.0]  # L2 regularization
    }

    xgb = XGBClassifier(
        scale_pos_weight=scale_pos_weight,
        eval_metric='logloss',
        random_state=42,
        n_jobs=-1
    )

    xgb_cv = RandomizedSearchCV(
        xgb, xgb_params, n_iter=50, scoring='f1', cv=3, n_jobs=-1, random_state=42
    )
    xgb_cv.fit(x_train_res, y_train_res)

    xgb_best = xgb_cv.best_estimator_
    y_prob = xgb_best.predict_proba(x_test_scaled)[:, 1]

    best_thresh, best_f1 = optimize_threshold(y_test, y_prob)
    y_pred = (y_prob >= best_thresh).astype(int)

    plot_conf_matrix(y_test, y_pred, "XGBoost")

    results['XGBoost'] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_prob),
        'threshold': best_thresh
    }

    models['XGBoost'] = xgb_best


# ==================================================================
# MODEL 4: LIGHTGBM
# ==================================================================
    print("\n LightGBM (Tuned)")
    lgbm = LGBMClassifier(
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )

    lgbm_params = {
        'n_estimators': [100, 200, 300],
        'max_depth': [4, 6, 8],
        'learning_rate': [0.01, 0.03, 0.05],
        'num_leaves': [31, 50],
        'subsample': [0.7, 0.8],
        'colsample_bytree': [0.7, 0.8],
        # 'min_child_samples': [10, 20, 30, 50],
        # 'reg_alpha': [0, 0.1, 0.5, 1.0],
        # 'reg_lambda': [0.5, 1.0, 2.0, 5.0]
    }

    lgbm_cv = RandomizedSearchCV(
        lgbm,
        lgbm_params,
        n_iter=50,
        scoring='f1',
        cv=3,
        n_jobs=-1,
        random_state=42
    )

    lgbm_cv.fit(x_train_res, y_train_res)

    lgbm_best = lgbm_cv.best_estimator_

    # Predict probabilities
    y_prob = lgbm_best.predict_proba(x_test_scaled)[:, 1]

    # Threshold optimization
    best_thresh, best_f1 = optimize_threshold(y_test, y_prob)
    y_pred = (y_prob >= best_thresh).astype(int)

    # Confusion Matrix
    plot_conf_matrix(y_test, y_pred, "LightGBM")

    # Store results
    results['LightGBM'] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_prob),
        'threshold': best_thresh
    }

    models['LightGBM'] = lgbm_best


    # ------------------------------------------------------------------
    # Final Comparison
    # ------------------------------------------------------------------
    comparison_df = pd.DataFrame(results).T
    print("\n FINAL MODEL COMPARISON")
    print(comparison_df)

    best_model_name = comparison_df['f1'].idxmax()
    print(f"\n BEST MODEL (by F1): {best_model_name}")

    # Return the best model, all models, and comparison dataframe
    return {
        "best_model_name": best_model_name,
        "best_model": models[best_model_name],
        "all_models": models,
        "comparison_df": comparison_df
    }

# Call the function
output = build_and_compare_churn_models_optimized(telco)

"""# **Telcom Recommendation Pipeline**

Customer Cluster
"""

def perform_clustering(telco, n_clusters=5):
    """Cluster customers based on usage patterns"""
    print("STEP 5: CUSTOMER CLUSTERING")


    # Select features for clustering
    clustering_features = []

    # Usage features
    if 'Tenure Months' in telco.columns:
        clustering_features.append('Tenure Months')
    if 'Monthly Charges' in telco.columns:
        clustering_features.append('Monthly Charges')
    if 'Total_Services' in telco.columns:
        clustering_features.append('Total_Services')

    # Encoded categorical features
    streaming_cols = [col for col in telco.columns if 'Streaming' in col and col.endswith('_encoded')]
    internet_cols = [col for col in telco.columns if 'Internet Service' in col and col.endswith('_encoded')]
    multiline_cols = [col for col in telco.columns if 'Multiple Lines' in col and col.endswith('_encoded')]

    clustering_features.extend(streaming_cols + internet_cols + multiline_cols)

    # Remove duplicates
    clustering_features = list(set(clustering_features))
    clustering_features = [f for f in clustering_features if f in telco.columns]

    print(f"\n Clustering Features ({len(clustering_features)}):")
    print(f"   {clustering_features}")

    # Prepare data
    X_cluster = telco[clustering_features].fillna(0)

    # Scale features
    scaler_cluster = StandardScaler()
    X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)
    print("\nâœ“ Features scaled for clustering")

    # Determine optimal k using elbow method
    print("\n Finding optimal number of clusters...")
    inertias = []
    silhouette_scores = []
    K_range = range(2, 8)

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_cluster_scaled)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans.labels_))

    print("\nInertia and Silhouette Scores:")
    for k, inertia, sil in zip(K_range, inertias, silhouette_scores):
        print(f"   k={k}: Inertia={inertia:.2f}, Silhouette={sil:.4f}")

    # Perform K-Means clustering with chosen k
    print(f"\n Performing K-Means with k={n_clusters}...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20, max_iter=300)
    telco['Cluster'] = kmeans.fit_predict(X_cluster_scaled)

    # Clustering Metrics
    sil_score = silhouette_score(X_cluster_scaled, telco['Cluster'])
    db_score = davies_bouldin_score(X_cluster_scaled, telco['Cluster'])
    ch_score = calinski_harabasz_score(X_cluster_scaled, telco['Cluster'])

    print("\n CLUSTERING METRICS:")
    print(f"   Silhouette Score: {sil_score:.4f} (higher is better)")
    print(f"   Davies-Bouldin Score: {db_score:.4f} (lower is better)")
    print(f"   Calinski-Harabasz Score: {ch_score:.2f} (higher is better)")

    # Analyze clusters
    print("\n CLUSTER ANALYSIS:")


    cluster_names = {
        0: 'Light Users',
        1: 'Heavy Streamers',
        2: 'Voice-Heavy Users',
        3: 'Budget Customers',
        4: 'Multi-Line Family Users'
    }

    for cluster_id in range(n_clusters):
        cluster_data = telco[telco['Cluster'] == cluster_id]
        cluster_name = cluster_names.get(cluster_id, f'Cluster {cluster_id}')

        print(f"\n{cluster_name} (Cluster {cluster_id}):")
        print(f"   Count: {len(cluster_data)} customers ({len(cluster_data)/len(telco)*100:.1f}%)")

        if 'Churn Value' in telco.columns:
            churn_rate = cluster_data['Churn Value'].mean()
            print(f"   Churn Rate: {churn_rate:.2%}")

        if 'Tenure Months' in telco.columns:
            print(f"   Avg Tenure: {cluster_data['Tenure Months'].mean():.1f} months")

        if 'Monthly Charges' in telco.columns:
            print(f"   Avg Monthly Charges: ${cluster_data['Monthly Charges'].mean():.2f}")

        if 'Total_Services' in telco.columns:
            print(f"   Avg Services: {cluster_data['Total_Services'].mean():.1f}")
    return telco, kmeans, scaler_cluster, clustering_features, cluster_names

telco, kmeans_model, scaler_cluster, clustering_features, cluster_names = perform_clustering(
    telco,
    n_clusters=5
)

"""# **Plan Recommendation Engine**"""

def create_plan_recommendations(telco, cluster_names):
    """Map clusters to recommended plans"""
    print("STEP: PLAN RECOMMENDATION ENGINE")

    # Define plan mapping based on cluster characteristics
    plan_mapping = {
        0: {  # Light Users
            'plan_name': 'Basic Saver Plan',
            'description': 'Essential services with minimal data',
            'features': ['Basic phone service', 'Limited data (5GB)', 'No streaming'],
            'price_range': '$35-45',
            'target': 'Low usage customers seeking affordability'
        },
        1: {  # Heavy Streamers
            'plan_name': 'Premium Streaming Plan',
            'description': 'Unlimited data with streaming perks',
            'features': ['Unlimited data', 'HD streaming', 'Multi-device support', 'Free streaming subscriptions'],
            'price_range': '$75-95',
            'target': 'High data users with streaming needs'
        },
        2: {  # Voice-Heavy Users
            'plan_name': 'Talk & Text Pro',
            'description': 'Optimized for voice and messaging',
            'features': ['Unlimited calls', 'Unlimited texts', 'Moderate data (10GB)', 'International calling'],
            'price_range': '$50-65',
            'target': 'Customers prioritizing voice communication'
        },
        3: {  # Budget Customers
            'plan_name': 'Economy Essential',
            'description': 'Maximum value, minimal cost',
            'features': ['Pay-as-you-go', 'Basic phone', 'Limited data (2GB)', 'No contracts'],
            'price_range': '$25-40',
            'target': 'Price-sensitive customers'
        },
        4: {  # Multi-Line Family Users
            'plan_name': 'Family Share Bundle',
            'description': 'Multi-line plan with shared benefits',
            'features': ['4+ lines', 'Shared data pool (50GB)', 'Family controls', 'Bundled discounts'],
            'price_range': '$85-110',
            'target': 'Families and households with multiple users'
        }
    }

    print("\n PLAN RECOMMENDATIONS BY CLUSTER:")


    for cluster_id, plan in plan_mapping.items():
        cluster_name = cluster_names.get(cluster_id, f'Cluster {cluster_id}')
        print(f"\n{cluster_name} â†’ {plan['plan_name']}")
        print(f"   Price: {plan['price_range']}")
        print(f"   Description: {plan['description']}")
        print(f"   Features:")
        for feature in plan['features']:
            print(f" {feature}")
        print(f"   Target: {plan['target']}")

    # Add plan recommendations to dataframe
    telco['Recommended_Plan'] = telco['Cluster'].map(lambda x: plan_mapping[x]['plan_name'])
    telco['Plan_Price_Range'] = telco['Cluster'].map(lambda x: plan_mapping[x]['price_range'])
    return telco, plan_mapping



cluster_names = {
        0: 'Light Users',
        1: 'Heavy Streamers',
        2: 'Voice-Heavy Users',
        3: 'Budget Customers',
        4: 'Multi-Line Family Users'
    }
telco, plan_mapping= create_plan_recommendations(telco, cluster_names)

"""# **Integrated Prediction Pipeline**"""

def predict_and_recommend(
    customer_data,
    churn_model,
    scaler_churn,
    feature_cols,
    cluster_model,
    scaler_cluster,
    clustering_features,
    plan_mapping,
    threshold=0.5
):
    """
    Complete pipeline:
    1. Predict churn probability
    2. Identify high-risk customers
    3. Cluster high-risk customers
    4. Recommend plans
    """

    print("STEP: INTEGRATED CHURN PREDICTION & PLAN RECOMMENDATION")

    # Work on a copy to avoid side effects
    results = customer_data.copy()

    # -------------------------------
    # Step 1: Churn Prediction
    # -------------------------------
    X_churn = results[feature_cols].fillna(0)
    X_churn_scaled = scaler_churn.transform(X_churn)
    churn_proba = churn_model.predict_proba(X_churn_scaled)[:, 1]

    results['Churn_Probability'] = churn_proba
    results['Risk_Category'] = np.where(
        churn_proba > threshold, 'High Risk', 'Low Risk'
    )

    print("\nCHURN PREDICTION SUMMARY")
    print(f"Customers analysed : {len(results)}")
    print(f"High-risk (> {threshold}) : {(churn_proba > threshold).sum()}")
    print(f"Low-risk (â‰¤ {threshold}) : {(churn_proba <= threshold).sum()}")

    # -------------------------------
    # Step 2: Clustering High-Risk Customers
    # -------------------------------
    high_risk_mask = results['Risk_Category'] == 'High Risk'

    if high_risk_mask.sum() > 0:
        print(f"\nClustering {high_risk_mask.sum()} high-risk customers...")

        high_risk_data = results.loc[high_risk_mask, clustering_features].fillna(0)
        X_cluster_scaled = scaler_cluster.transform(high_risk_data)
        clusters = cluster_model.predict(X_cluster_scaled)

        results.loc[high_risk_mask, 'Cluster'] = clusters
        results.loc[high_risk_mask, 'Recommended_Plan'] = [
            plan_mapping[c]['plan_name'] for c in clusters
        ]
        results.loc[high_risk_mask, 'Recommendation_Reason'] = 'Churn Prevention'

    else:
        print("\nNo high-risk customers detected â€” skipping clustering.")

    # -------------------------------
    # Step 3: Low-Risk Customers
    # -------------------------------
    low_risk_mask = results['Risk_Category'] == 'Low Risk'
    results.loc[low_risk_mask, 'Recommended_Plan'] = 'Current Plan (Satisfied)'
    results.loc[low_risk_mask, 'Recommendation_Reason'] = 'Low Churn Risk'

    # -------------------------------
    # Final Summary
    # -------------------------------
    print("\nRECOMMENDATION SUMMARY")
    print(results['Recommended_Plan'].value_counts())

    # Display preview (Jupyter / Colab friendly)
    print(
        results[
            [
                'Churn_Probability',
                'Risk_Category',
                'Cluster',
                'Recommended_Plan',
                'Recommendation_Reason'
            ]
        ].head(10)
    )

    return results


final_results = predict_and_recommend(
    customer_data=telco,
    churn_model=best_model,
    scaler_churn=scaler,
    feature_cols=feature_cols,
    cluster_model=kmeans_model,
    scaler_cluster=scaler_cluster,
    clustering_features=clustering_features,
    plan_mapping=plan_mapping,
    threshold=0.5
)

"""# **Sample Prediction and Business Insights**"""

def generate_business_insights(telco):
    """Generate actionable business insights"""

    print("STEP 8: BUSINESS INSIGHTS & RECOMMENDATIONS")


    # High-risk customer analysis
    if 'Churn_Probability' in telco.columns:
        high_risk = telco[telco['Churn_Probability'] > 0.5]
        print(f"\n HIGH-RISK CUSTOMERS:")
        print(f"   Count: {len(high_risk)} ({len(high_risk)/len(telco)*100:.1f}%)")

        if len(high_risk) > 0 and 'Monthly Charges' in telco.columns:
            revenue_at_risk = high_risk['Monthly Charges'].sum() * 12
            print(f"   Annual Revenue at Risk: ${revenue_at_risk:,.2f}")

    # Plan recommendation distribution
    if 'Recommended_Plan' in telco.columns:
        print(f"\n PLAN RECOMMENDATIONS:")
        plan_dist = telco['Recommended_Plan'].value_counts()
        for plan, count in plan_dist.items():
            print(f"   {plan}: {count} customers ({count/len(telco)*100:.1f}%)")

    # Cluster-wise churn rate
    if 'Cluster' in telco.columns and 'Churn Value' in telco.columns:
        print(f"\n CHURN RATE BY CLUSTER:")
        cluster_churn = telco.groupby('Cluster')['Churn Value'].agg(['mean', 'count'])
        cluster_churn['churn_rate_%'] = cluster_churn['mean'] * 100
        print(cluster_churn)

    # Key recommendations
    print(f"\n KEY BUSINESS RECOMMENDATIONS:")
    print("   1. Prioritize intervention for Heavy Streamers cluster (high churn)")
    print("   2. Offer contract upgrades to Month-to-month customers")
    print("   3. Bundle services for Budget Customers to increase CLTV")
    print("   4. Implement loyalty rewards for Multi-Line Family users")
    print("   5. Provide personalized retention offers based on cluster characteristics")

    # Sample predictions
    print(f"\n SAMPLE CUSTOMER RECOMMENDATIONS:")


    if 'Churn_Probability' in telco.columns:
        sample = telco[telco['Churn_Probability'] > 0.5].head(5)

        for idx, row in sample.iterrows():
            print(f"\nCustomer ID: {idx}")
            if 'Tenure Months' in row.index:
                print(f"   Tenure: {row['Tenure Months']} months")
            if 'Monthly Charges' in row.index:
                print(f"   Monthly Charges: ${row['Monthly Charges']:.2f}")
            if 'Churn_Probability' in row.index:
                print(f"   Churn Risk: {row['Churn_Probability']:.2%}")
            if 'Cluster' in row.index and not pd.isna(row['Cluster']):
                print(f"   Segment: Cluster {int(row['Cluster'])}")
            if 'Recommended_Plan' in row.index:
                print(f" Recommended: {row['Recommended_Plan']}")

# def interactive_churn_prediction(
#     churn_model,
#     scaler_churn,
#     feature_cols,
#     cluster_model,
#     scaler_cluster,
#     clustering_features,
#     plan_mapping,
#     threshold=0.5
# ):
#     """
#     Interactive churn prediction & recommendation system
#     """

#     print("\nINTERACTIVE CHURN PREDICTION SYSTEM")
#     print("-" * 50)

#     # -----------------------------
#     # Step 1: Collect user inputs
#     # -----------------------------
#     customer = {}

#     for feature in feature_cols:
#         value = input(f"Enter value for '{feature}': ")
#         customer[feature] = float(value)

#     customer_df = pd.DataFrame([customer])

#     # -----------------------------
#     # Step 2: Predict churn
#     # -----------------------------
#     X_scaled = scaler_churn.transform(customer_df)
#     churn_prob = churn_model.predict_proba(X_scaled)[0, 1]

#     print(f"\nChurn Probability: {churn_prob:.2%}")

#     # -----------------------------
#     # Step 3: Risk decision
#     # -----------------------------
#     if churn_prob <= threshold:
#         print("\nCustomer is LOW RISK of churn.")
#         print("Recommendation: Maintain current plan and engagement.")
#         return

#     print("\nCustomer is HIGH RISK of churn.")

#     # -----------------------------
#     # Step 4: Cluster & recommend
#     # -----------------------------
#     cluster_input = customer_df[clustering_features]
#     cluster_scaled = scaler_cluster.transform(cluster_input)
#     cluster = cluster_model.predict(cluster_scaled)[0]

#     plan = plan_mapping[cluster]

#     print("\nRECOMMENDED RETENTION PLAN")
#     print(f"Plan Name: {plan['plan_name']}")
#     print(f"Description: {plan['description']}")
#     print(f"Price Range: {plan['price_range']}")
#     print("Features:")
#     for f in plan['features']:
#         print(f" - {f}")



# interactive_churn_prediction(
#     churn_model=best_model,
#     scaler_churn=scaler,
#     feature_cols=feature_cols,
#     cluster_model=kmeans_model,
#     scaler_cluster=scaler_cluster,
#     clustering_features=clustering_features,
#     plan_mapping=plan_mapping
# )





def interactive_churn_prediction(
    churn_model,
    scaler_churn,
    feature_cols,
    cluster_model,
    scaler_cluster,
    clustering_features,
    plan_mapping,
    threshold=0.5,
    top_n_features=15
):
    """
    Interactive churn prediction & recommendation system
    Uses only the most important features for user input
    """

    print("\n" + "="*50)
    print("INTERACTIVE CHURN PREDICTION SYSTEM")
    print("="*50)

    # -----------------------------
    # Step 0: Identify top important features
    # -----------------------------
    print(f"\n  Using TOP {top_n_features} most important features only")

    # Get feature importances from the model
    if hasattr(churn_model, 'feature_importances_'):
        importances = churn_model.feature_importances_
    elif hasattr(churn_model, 'coef_'):
        # For logistic regression, use absolute coefficients
        importances = np.abs(churn_model.coef_[0])
    else:
        # Default to all features if can't determine importance
        importances = np.ones(len(feature_cols))

    # Create importance dataframe
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': importances
    }).sort_values('importance', ascending=False)

    # Select top N features
    top_features = importance_df.head(top_n_features)['feature'].tolist()

    print(f"Top {top_n_features} features:")
    for i, feat in enumerate(top_features, 1):
        print(f"  {i:2d}. {feat}")

    # -----------------------------
    # Step 1: Collect user inputs for IMPORTANT features only
    # -----------------------------
    print(f"\n{'â”€'*50}")
    print("STEP 1: CUSTOMER INFORMATION (Enter values below)")
    print(f"{'â”€'*50}")

    customer = {}

    # For each important feature, get user input
    for feature in top_features:
        # Provide helpful context for each feature
        if feature == 'Tenure Months':
            print(f"\nâ†³ {feature} (Customer's tenure in months, e.g., 12): ", end="")
        elif feature == 'Monthly Charges':
            print(f"\nâ†³ {feature} (Monthly bill amount, e.g., 70.50): ", end="")
        elif feature == 'Contract_Month-to-month':
            print(f"\nâ†³ {feature} (1 for Month-to-month, 0 for contract): ", end="")
        elif feature == 'Internet_Service_Fiber optic':
            print(f"\nâ†³ {feature} (1 for Fiber optic, 0 for other): ", end="")
        elif feature == 'Payment_Method_Electronic check':
            print(f"\nâ†³ {feature} (1 for Electronic check, 0 for other): ", end="")
        elif 'Total_Services' in feature:
            print(f"\nâ†³ {feature} (Number of services, e.g., 3): ", end="")
        else:
            print(f"\nâ†³ {feature}: ", end="")

        value = input()
        try:
            customer[feature] = float(value)
        except ValueError:
            # Provide default values for common features if invalid input
            defaults = {
                'Tenure Months': 12,
                'Monthly Charges': 70.50,
                'Contract_Month-to-month': 0,
                'Payment_Method_Electronic check': 0,
                'Total_Services': 3
            }
            customer[feature] = defaults.get(feature, 0.0)
            print(f"  Using default: {customer[feature]}")

    # Create a complete feature vector with zeros for non-important features
    customer_complete = {}
    for feature in feature_cols:
        if feature in customer:  # Important feature from user input
            customer_complete[feature] = customer[feature]
        else:  # Less important feature - set to median or zero
            # Try to get sensible default based on feature type
            if 'Contract_' in feature:
                customer_complete[feature] = 0.0
            elif 'Payment_Method_' in feature:
                customer_complete[feature] = 0.0
            elif 'Internet_Service_' in feature:
                customer_complete[feature] = 0.0
            elif feature in ['Gender', 'Senior Citizen', 'Partner', 'Dependents']:
                customer_complete[feature] = 0.0  # Default to most common
            else:
                customer_complete[feature] = 0.0  # Default to zero

    customer_df = pd.DataFrame([customer_complete])

    # -----------------------------
    # Step 2: Predict churn
    # -----------------------------
    print(f"\n{'â”€'*50}")
    print("STEP 2: CHURN PREDICTION")
    print(f"{'â”€'*50}")

    X_scaled = scaler_churn.transform(customer_df)
    churn_prob = churn_model.predict_proba(X_scaled)[0, 1]

    # Display churn probability with visual indicator
    print(f"\n Churn Probability: {churn_prob:.2%}")
    if churn_prob > 0.7:
        print(" HIGH RISK")
    elif churn_prob > 0.4:
        print("  MEDIUM RISK")
    else:
        print(" LOW RISK")

    # -----------------------------
    # Step 3: Risk decision
    # -----------------------------
    if churn_prob <= threshold:
        print(f"\n Customer is LOW RISK of churn (<{threshold:.0%}).")
        print(" Recommendation: Maintain current plan with periodic check-ins.")
        return

    print(f"\n  Customer is HIGH RISK of churn (>{threshold:.0%})")
    print(f"   Probability: {churn_prob:.2%}")

    # -----------------------------
    # Step 4: Cluster & recommend
    # -----------------------------
    print(f"\n{'â”€'*50}")
    print("STEP 3: PERSONALIZED PLAN RECOMMENDATION")
    print(f"{'â”€'*50}")

    # Prepare clustering features (using defaults for missing ones)
    cluster_input_data = {}
    for feature in clustering_features:
        if feature in customer:
            cluster_input_data[feature] = customer[feature]
        else:
            # Provide sensible defaults for clustering features
            if 'Tenure' in feature:
                cluster_input_data[feature] = 12
            elif 'Charges' in feature:
                cluster_input_data[feature] = 70.50
            elif 'Services' in feature:
                cluster_input_data[feature] = 3
            else:
                cluster_input_data[feature] = 0

    cluster_input_df = pd.DataFrame([cluster_input_data])
    cluster_scaled = scaler_cluster.transform(cluster_input_df)
    cluster = cluster_model.predict(cluster_scaled)[0]

    plan = plan_mapping.get(cluster, {
        'plan_name': 'Standard Retention Plan',
        'description': 'General retention offer',
        'price_range': '$50-70',
        'features': ['Personalized discount', 'Service review', 'Loyalty bonus']
    })

    print(f"\n Recommended Plan: {plan['plan_name']}")
    print(f" Description: {plan['description']}")
    print(f" Price Range: {plan['price_range']}")
    print(" Features:")
    for f in plan['features']:
        print(f"   â€¢ {f}")

    # -----------------------------
    # Step 5: Additional insights
    # -----------------------------
    print(f"\n{'â”€'*50}")
    print("STEP 4: KEY INSIGHTS & ACTION ITEMS")
    print(f"{'â”€'*50}")

    # Identify key risk factors from user input
    risk_factors = []
    if customer.get('Contract_Month-to-month', 0) == 1:
        risk_factors.append("Month-to-month contract")
    if customer.get('Payment_Method_Electronic check', 0) == 1:
        risk_factors.append("Electronic check payment")
    if customer.get('Tenure Months', 12) < 12:
        risk_factors.append(f"Short tenure ({customer.get('Tenure Months', 12)} months)")
    if customer.get('Monthly Charges', 0) > 80:
        risk_factors.append(f"High monthly charges (${customer.get('Monthly Charges', 0):.2f})")

    if risk_factors:
        print(f"\n Key Risk Factors Identified:")
        for factor in risk_factors:
            print(f"   â€¢ {factor}")

    print(f"\n Suggested Actions:")
    print(f"   1. Offer contract upgrade incentive")
    print(f"   2. Switch to automatic payment method")
    print(f"   3. Provide personalized service review")
    print(f"   4. Consider targeted discount")

    return {
        'churn_probability': churn_prob,
        'risk_category': 'High' if churn_prob > threshold else 'Low',
        'recommended_plan': plan['plan_name'],
        'cluster': int(cluster) if cluster is not None else None,
        'key_risk_factors': risk_factors
    }


# Call the function with your models
interactive_churn_prediction(
    churn_model=best_model,
    scaler_churn=scaler,
    feature_cols=feature_cols,
    cluster_model=kmeans_model,
    scaler_cluster=scaler_cluster,
    clustering_features=clustering_features,
    plan_mapping=plan_mapping,
    threshold=0.5,
    top_n_features=10  # You can adjust this number
)

"""# **Saving Model For Deployment**"""

import joblib
import os

# ---------------------------
# CREATE DIRECTORIES
# ---------------------------
os.makedirs("models", exist_ok=True)
os.makedirs("config", exist_ok=True)

# ---------------------------
# SAVE CHURN MODEL ARTIFACTS
# ---------------------------
joblib.dump(best_model, "models/churn_model.pkl")
joblib.dump(scaler, "models/churn_scaler.pkl")

# ---------------------------
# SAVE CLUSTERING ARTIFACTS
# ---------------------------
joblib.dump(kmeans_model, "models/kmeans_model.pkl")
joblib.dump(scaler_cluster, "models/cluster_scaler.pkl")

# ---------------------------
# SAVE CONFIG FILES
# ---------------------------
joblib.dump(feature_cols, "config/feature_cols.pkl")
joblib.dump(clustering_features, "config/clustering_features.pkl")
joblib.dump(plan_mapping, "config/plan_mapping.pkl")

print("All models and configs saved successfully")